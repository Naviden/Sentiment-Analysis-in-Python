{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4749fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas nltk scikit-learn tensorflow vaderSentiment tensorflow_datasets textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac9a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16eb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f91eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "train_data, test_data = dataset['train'], dataset['test']\n",
    "train_df = tfds.as_dataframe(train_data, info)\n",
    "test_df = tfds.as_dataframe(test_data, info)\n",
    "\n",
    "# Decode bytes to string\n",
    "train_df['text'] = train_df['text'].str.decode('utf-8')\n",
    "test_df['text'] = test_df['text'].str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c76fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\n",
      "VADER Polarity Scores: {'neg': 0.072, 'neu': 0.783, 'pos': 0.144, 'compound': 0.9213}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample application on a single review\n",
    "sample_review = test_df['text'].iloc[0]\n",
    "print(sample_review)\n",
    "scores = analyzer.polarity_scores(sample_review)\n",
    "print(\"VADER Polarity Scores:\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b1fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.8516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ae277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum number of words to keep, based on word frequency\n",
    "max_words = 10000\n",
    "# Max number of words in each complaint.\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef0c5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128, input_length=max_len))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dee2ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 32s 768ms/step - loss: 0.6573 - accuracy: 0.6181 - val_loss: 0.6011 - val_accuracy: 0.7220\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 31s 765ms/step - loss: 0.4661 - accuracy: 0.8094 - val_loss: 0.3825 - val_accuracy: 0.8344\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 30s 746ms/step - loss: 0.2755 - accuracy: 0.8898 - val_loss: 0.3257 - val_accuracy: 0.8668\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 31s 763ms/step - loss: 0.1954 - accuracy: 0.9288 - val_loss: 0.3433 - val_accuracy: 0.8640\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 30s 757ms/step - loss: 0.1476 - accuracy: 0.9517 - val_loss: 0.3415 - val_accuracy: 0.8686\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 30s 763ms/step - loss: 0.1184 - accuracy: 0.9630 - val_loss: 0.3795 - val_accuracy: 0.8660\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 30s 757ms/step - loss: 0.1008 - accuracy: 0.9682 - val_loss: 0.3994 - val_accuracy: 0.8578\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 31s 764ms/step - loss: 0.0880 - accuracy: 0.9728 - val_loss: 0.4968 - val_accuracy: 0.8594\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 30s 762ms/step - loss: 0.0881 - accuracy: 0.9725 - val_loss: 0.5010 - val_accuracy: 0.8606\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 30s 755ms/step - loss: 0.0684 - accuracy: 0.9804 - val_loss: 0.4930 - val_accuracy: 0.8624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(padded_sequences,\n",
    "                                                  train_df['label'],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26516d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 12s - loss: 0.5234 - accuracy: 0.8476 - 12s/epoch - 16ms/step\n",
      "Test Loss: 0.523405909538269\n",
      "Test Accuracy: 0.8476399779319763\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the test data\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_padded, test_df['label'], verbose=2)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da5c3e",
   "metadata": {},
   "source": [
    "Notes\n",
    "- **Preprocessing**: The maximum number of words (max_words) and the maximum sequence length (max_len) are parameters that you can tune. Adjusting these can affect both the performance of your model and the training time.\n",
    "\n",
    "- **Model Architecture**: The LSTM layer parameters (number of units, dropout) and the embedding dimension are also tunable. Experimenting with different values can help improve your model's accuracy.\n",
    "\n",
    "- **Training**: The number of epochs and batch size are important parameters that influence how well your model learns. Too few epochs might underfit, while too many might overfit, especially without proper regularization or dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "sentiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
